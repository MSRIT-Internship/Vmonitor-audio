{"cells":[{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-03T15:12:25.093994Z","iopub.status.busy":"2024-06-03T15:12:25.093303Z","iopub.status.idle":"2024-06-03T15:12:25.099660Z","shell.execute_reply":"2024-06-03T15:12:25.098537Z","shell.execute_reply.started":"2024-06-03T15:12:25.093962Z"},"trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import os\n","\n","# List of important Directories\n","ROOT = '/kaggle/input'\n","DATASET_DIR = '/kaggle/input/baby-sounds/Infant_Cry_Sounds_Kaggle'\n","AUDIO_DIR = DATASET_DIR + \"/data\"\n","METADATA_DIR = DATASET_DIR + \"/metadata\"\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:12:25.102370Z","iopub.status.busy":"2024-06-03T15:12:25.101971Z","iopub.status.idle":"2024-06-03T15:12:25.117432Z","shell.execute_reply":"2024-06-03T15:12:25.116460Z","shell.execute_reply.started":"2024-06-03T15:12:25.102337Z"},"trusted":true},"outputs":[],"source":["# Making metadata (ignore this)\n","class_indexes = {\n","#     \"hungry\": 0,\n","#     \"tired\": 1,\n","#     \"uncomfortable\": 2,\n","    \"cry\": 0,\n","    \"laugh\": 1,\n","    \"noise\": 2,\n","    \"silence\": 3\n","}"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:55:54.418079Z","iopub.status.busy":"2024-06-03T15:55:54.417696Z","iopub.status.idle":"2024-06-03T15:55:54.435174Z","shell.execute_reply":"2024-06-03T15:55:54.434049Z","shell.execute_reply.started":"2024-06-03T15:55:54.418052Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","import torchaudio\n","\n","class BabyDataset(Dataset):\n","    def __init__(self, annotations_file, audio_dir, transformation, target_sr, num_of_samples, device):\n","        self.annotations = pd.read_csv(annotations_file) # metadata file\n","        self.audio_dir = audio_dir # audio directory\n","        self.device = device\n","        self.transformation = transformation.to(device) # mel-spectrogram (function)\n","        self.target_sr = target_sr # sample rate\n","        self.num_of_samples = num_of_samples\n","    \n","    def __len__(self): # returns the total number of audio_files in the dataset\n","        return len(self.annotations) \n","    \n","    \n","    # Preprocessing audio whilst getting the audio\n","    def __getitem__(self, index): # returns the signal(tensor) and label name of the particular audio\n","        audio_sample_path = self._get_audio_sample_path(index)\n","        label = self._get_audio_sample_label(index)\n","        signal, sr = torchaudio.load(audio_sample_path)\n","        signal = signal.to(self.device) # send signal to particular device\n","        signal = self._resample_if_necessary(signal, sr) # maintains all audios with uniform sample rate\n","        signal = self._mix_down_if_necessary(signal) # makes audio mono-channel\n","        signal = self._cut_if_necessary(signal) # truncates remaining samples if samples are more than expected num of samples\n","        signal = self._right_pad_if_necessary(signal) # maintains uniform number of samples if samples are lesser than expected num of samples\n","        signal = self.transformation(signal)\n","        return signal, label\n","    \n","    def _get_audio_sample_path(self, index): # returns a path string of the particular audio\n","        path = os.path.join(self.audio_dir, self.annotations.iloc[index, 3], self.annotations.iloc[index, 1])\n","        return path\n","    \n","    def _get_audio_sample_label(self, index): \n","        return self.annotations.iloc[index, 1]\n","    \n","    def _mix_down_if_necessary(self, signal):\n","        if signal.shape[0] > 1:\n","            signal = torch.mean(\n","                signal,\n","                dim=0,\n","                keepdim=True\n","            )\n","        return signal\n","    \n","    def _resample_if_necessary(self, signal, sr):\n","        if sr != self.target_sr:\n","            resampler = torchaudio.transforms.Resample(sr, self.target_sr).to(self.device)\n","            signal = resampler(signal)\n","        return signal\n","    \n","    def _cut_if_necessary(self, signal):\n","        if signal.shape[1] > self.num_of_samples:\n","            signal = signal[:, :self.num_of_samples]\n","        return signal\n","    \n","    def _right_pad_if_necessary(self, signal):\n","        signal_length = signal.shape[1]\n","        if signal.shape[1] < self.num_of_samples:\n","            num_of_missing_samples = self.num_of_samples - signal_length\n","            last_dim_padding = (0, num_of_missing_samples)\n","            signal = torch.nn.functional.pad(signal, last_dim_padding)\n","        return signal\n","            \n","    "]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:59:59.179065Z","iopub.status.busy":"2024-06-03T15:59:59.178139Z","iopub.status.idle":"2024-06-03T15:59:59.214702Z","shell.execute_reply":"2024-06-03T15:59:59.213585Z","shell.execute_reply.started":"2024-06-03T15:59:59.179029Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["USING DEVICE: cuda\n","before: torch.Size([1, 221376])\n","after: torch.Size([1, 22050])\n","(tensor([[[1.4614e-03, 1.0155e-03, 2.9039e-02,  ..., 1.0453e-01,\n","          4.7356e-02, 2.5281e+00],\n","         [3.1175e-06, 4.5162e-07, 4.4379e-02,  ..., 4.3996e-02,\n","          2.0307e-01, 1.9964e+00],\n","         [5.0374e-07, 3.4675e-07, 3.4060e-02,  ..., 2.9633e-02,\n","          7.0934e-02, 1.5329e+00],\n","         ...,\n","         [4.6024e-08, 4.1675e-08, 9.8247e-06,  ..., 4.8067e-03,\n","          2.5472e-03, 1.0527e-02],\n","         [1.0236e-07, 2.8940e-08, 7.1711e-06,  ..., 3.0330e-02,\n","          1.5261e-02, 3.2795e-02],\n","         [7.4341e-08, 3.1492e-08, 4.7825e-06,  ..., 2.0047e-01,\n","          2.2641e-01, 1.6230e-01]]], device='cuda:0'), '1-187207-A.ogg')\n"]}],"source":["if __name__ == '__main__':\n","    ANNOTATIONS_FILE  = METADATA_DIR+\"/metadata.csv\"\n","    SAMPLE_RATE = 22050\n","    NUM_OF_SAMPLES = 22050\n","    \n","    \n","    # check if gpu is available\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    print(\"USING DEVICE:\", device)\n","    \n","    mel_spectrogram = torchaudio.transforms.MelSpectrogram( # returns a function\n","        sample_rate=SAMPLE_RATE,\n","        n_fft=1024,\n","        hop_length=512,\n","        n_mels=64\n","    )\n","    baby = BabyDataset( # initializing the Babyset Object\n","        ANNOTATIONS_FILE,\n","        AUDIO_DIR,\n","        mel_spectrogram,\n","        SAMPLE_RATE,\n","        NUM_OF_SAMPLES,\n","        device\n","    )\n","    \n","    # pre-processed and ready-to-go audio can be retrieved by getting item from baby obj using index: baby[index]\n","    print(baby[0])"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5141993,"sourceId":8595368,"sourceType":"datasetVersion"}],"dockerImageVersionId":30715,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
